# Research-reimplementations
This GitHub repository showcases from-scratch re-implementations of groundbreaking deep learning models, built primarily using PyTorch. This project focused on translating academic research papers into functional code to achieve a granular understanding of neural network engineering and training dynamics.

# **[AlexNet (2012)](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)**: 
Reproduced the convolutional architecture from "ImageNet Classification with Deep CNNs" in PyTorch.
Implemented core features: Convolutional layers, ReLU activations, Local Response Normalization, Dropout, and overlapping Max-Pooling.
> Future scope: optimize the accuracy of training

# **[Attention is all you need (2017)](https://arxiv.org/abs/1706.03762)**: 
Built the complete encoderâ€“decoder architecture based on the "Attention Is All You Need" paper.
Included foundational mechanisms: Multi-Head Self-Attention, Positional Encoding, and Scaled Dot-Product Attention.
